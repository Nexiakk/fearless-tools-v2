name: Champion Data Scraper

on:
  # Manual trigger - allows you to run whenever you want
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to deploy to"
        required: false
        default: "production"
        type: choice
        options:
          - development
          - staging
          - production

  # Scheduled execution every 24 hours
  schedule:
    - cron: "0 0 * * *" # Runs daily at midnight UTC

  # Run on push to main branch for testing
  push:
    branches: [main, master]
    paths:
      - "aws-scraping/**"

env:
  ENVIRONMENT: ${{ github.event.inputs.environment || 'production' }}

jobs:
  # Pre-deployment validation
  validate:
    runs-on: ubuntu-latest
    outputs:
      should_deploy: ${{ steps.check.outputs.should_deploy }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Check if deployment is needed
        id: check
        run: |
          # Check if there are actual changes to aws-scraping code
          if git diff --name-only HEAD~1 | grep -q "^aws-scraping/"; then
            echo "should_deploy=true" >> $GITHUB_OUTPUT
            echo "üîÑ Changes detected in aws-scraping directory"
          elif [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "should_deploy=true" >> $GITHUB_OUTPUT
            echo "üîÑ Manual trigger - proceeding with deployment"
          elif [ "${{ github.event_name }}" = "schedule" ]; then
            echo "should_deploy=true" >> $GITHUB_OUTPUT
            echo "üîÑ Scheduled run - proceeding with scraping"
          else
            echo "should_deploy=false" >> $GITHUB_OUTPUT
            echo "‚è≠Ô∏è No changes detected, skipping deployment"
          fi

  # Main scraping job
  scrape:
    runs-on: ubuntu-latest
    needs: validate
    if: needs.validate.outputs.should_deploy == 'true'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('aws-scraping/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          cd aws-scraping
          pip install -r requirements.txt

      - name: Validate configuration
        run: |
          cd aws-scraping
          python -c "
          from scraper.config import get_config
          config = get_config()
          print('‚úÖ Configuration loaded successfully')
          print(f'Environment: {config.environment}')
          print(f'Debug: {config.debug}')
          "

      - name: Run tests
        run: |
          cd aws-scraping
          # Install test dependencies
          pip install pytest
          # Run tests if they exist
          if [ -d "tests" ]; then
            python -m pytest tests/ -v --tb=short
          else
            echo "‚ö†Ô∏è No tests directory found, skipping tests"
          fi

      - name: Run champion scraper
        id: scrape
        timeout-minutes: 20
        env:
          FIREBASE_SERVICE_ACCOUNT_KEY: ${{ secrets.FIREBASE_SERVICE_ACCOUNT_KEY }}
          ENVIRONMENT: ${{ env.ENVIRONMENT }}
          DEBUG: ${{ env.ENVIRONMENT == 'development' }}
        run: |
          cd aws-scraping

          # Set start time for monitoring
          echo "SCRAPE_START_TIME=$(date +%s)" >> $GITHUB_ENV

          # Show initial progress
          echo "üöÄ Starting champion scraper..."
          echo "‚è∞ Timeout: 20 minutes"

          # Run with progress monitoring
          (
            # Show progress every 30 seconds
            for i in {1..40}; do
              if [ -f "scraper_output.log" ]; then
                lines=$(wc -l < scraper_output.log 2>/dev/null || echo "0")
                echo "üìä Progress: $lines lines logged (checkpoint $i/40)" >&2
              fi
              sleep 30
            done
          ) &
          PROGRESS_PID=$!

          # Run the scraper with timeout
          timeout 20m python lambda_function.py > scraper_output.log 2>&1 &
          SCRAPER_PID=$!
          wait $SCRAPER_PID
          EXIT_CODE=$?

          # Kill progress monitor
          kill $PROGRESS_PID 2>/dev/null || true

          # Report results
          if [ $EXIT_CODE -eq 0 ]; then
            echo "SCRAPE_EXIT_CODE=0" >> $GITHUB_ENV
            echo "‚úÖ Scraping script completed successfully"
          elif [ $EXIT_CODE -eq 124 ]; then
            echo "SCRAPE_EXIT_CODE=124" >> $GITHUB_ENV
            echo "‚è∞ Scraping script timed out after 20 minutes"
          else
            echo "SCRAPE_EXIT_CODE=$EXIT_CODE" >> $GITHUB_ENV
            echo "‚ö†Ô∏è Scraping script exited with code $EXIT_CODE"
          fi

          # Show final log stats
          if [ -f "scraper_output.log" ]; then
            lines=$(wc -l < scraper_output.log)
            size=$(du -h scraper_output.log | cut -f1)
            echo "üìÑ Log file: $lines lines, $size"
            # Show last 10 lines of log
            echo "üìã Last 10 lines of log:"
            tail -10 scraper_output.log
          fi

          # Check if summary was created by the script
          if [ -f "scrape_summary.json" ]; then
            echo "SCRAPE_STATUS=success" >> $GITHUB_ENV
            echo "‚úÖ Summary file created by script"
          else
            echo "SCRAPE_STATUS=completed" >> $GITHUB_ENV
            echo "‚ö†Ô∏è No summary file - script may have issues"
          fi
        continue-on-error: true

      - name: Generate deployment summary
        id: summary
        run: |
          # Calculate duration
          end_time=$(date +%s)
          start_time=${{ env.SCRAPE_START_TIME }}
          duration=$((end_time - start_time))

          # Check if script created summary
          if [ -f "aws-scraping/scrape_summary.json" ]; then
            # Use the summary created by the script
            cp aws-scraping/scrape_summary.json scrape_summary.json
            echo "Using summary created by script"
          else
            # Create fallback summary
            cat << EOF > scrape_summary.json
            {
              "timestamp": "$(date -Iseconds)",
              "environment": "${{ env.ENVIRONMENT }}",
              "status": "${{ env.SCRAPE_STATUS }}",
              "exit_code": ${{ env.SCRAPE_EXIT_CODE }},
              "duration_seconds": $duration,
              "commit_sha": "${{ github.sha }}",
              "run_id": "${{ github.run_id }}",
              "trigger": "${{ github.event_name }}",
              "note": "Fallback summary - script did not create summary file"
            }
            EOF
            echo "Created fallback summary"
          fi

          echo "summary=$(cat scrape_summary.json | jq -c .)" >> $GITHUB_OUTPUT

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-logs-${{ github.run_id }}
          path: |
            aws-scraping/scraper.log
            aws-scraping/scraper_output.log
            scrape_summary.json
          retention-days: 7

  # Notification job
  notify:
    runs-on: ubuntu-latest
    needs: [validate, scrape]
    if: always() && (needs.scrape.result == 'success' || needs.scrape.result == 'failure')

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download logs
        uses: actions/download-artifact@v4
        with:
          name: scrape-logs-${{ github.run_id }}

      - name: Send notification
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK_URL }}
          DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK_URL }}
        run: |
          # Read summary
          if [ -f "scrape_summary.json" ]; then
            summary=$(cat scrape_summary.json)
            status=$(echo $summary | jq -r '.status')
            duration=$(echo $summary | jq -r '.duration_seconds')
            environment=$(echo $summary | jq -r '.environment')
          else
            status="unknown"
            duration="unknown"
            environment="${{ env.ENVIRONMENT }}"
          fi

          # Create notification message
          if [ "$status" = "success" ]; then
            message="‚úÖ Champion data scraping completed successfully\n‚Ä¢ Environment: $environment\n‚Ä¢ Duration: ${duration}s\n‚Ä¢ Run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          else
            message="‚ùå Champion data scraping failed\n‚Ä¢ Environment: $environment\n‚Ä¢ Duration: ${duration}s\n‚Ä¢ Run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n‚Ä¢ Check logs for details"
          fi

          # Send to Slack if webhook exists
          if [ -n "$SLACK_WEBHOOK" ]; then
            curl -X POST -H 'Content-type: application/json' \
              --data "{\"text\":\"$message\"}" \
              $SLACK_WEBHOOK
          fi

          # Send to Discord if webhook exists
          if [ -n "$DISCORD_WEBHOOK" ]; then
            curl -X POST -H 'Content-type: application/json' \
              --data "{\"content\":\"$message\"}" \
              $DISCORD_WEBHOOK
          fi

  # Health check job (runs after successful scraping)
  health_check:
    runs-on: ubuntu-latest
    needs: scrape
    if: needs.scrape.result == 'success'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Run health checks
        run: |
          echo "üè• Running post-deployment health checks..."

          # Check if Firebase data is accessible (basic connectivity test)
          # This would be expanded based on your specific health check needs

          echo "‚úÖ Health checks completed"

  # Rollback job (manual trigger only)
  rollback:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'rollback'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Rollback procedure
        run: |
          echo "üîÑ Performing rollback procedure..."
          # Implement rollback logic here
          # This could involve restoring from backup or previous deployment

          echo "‚úÖ Rollback completed"
