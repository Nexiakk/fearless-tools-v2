#!/usr/bin/env python3
"""
Manual Champion Scraper

Allows running the champion data scraper locally for specific champions,
without needing to trigger the AWS Lambda function.

Usage:
    python manual_scraper.py champion1 champion2 ... [options]

Examples:
    python manual_scraper.py Aatrox Ahri
    python manual_scraper.py "K'Sante" Jinx --patch 15.5
    python manual_scraper.py --all  # Scrape all champions
    python manual_scraper.py --missing  # Scrape only missing champions

Options:
    --all           Scrape all champions
    --missing       Scrape only champions not in database
    --patch PATCH   Override patch version (default: latest)
    --dry-run       Show what would be scraped without actually doing it
    --help          Show this help message
"""

import os
import sys
import argparse
import json
import time
import random
from datetime import datetime

# Add scraper directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'scraper'))

from scraper.lolalytics_build_scraper import LolalyticsBuildScraper
from scraper.wiki_scraper import scrape_champion_abilities
from scraper.main import (
    check_patch_viability,
    normalize_patch_for_lolalytics,
    get_display_name,
    get_champion_id,
    get_champion_image_name,
    encode_champion_name_for_lolalytics,
    get_champion_list,
    get_current_patch
)

# Firebase imports (optional - will work without Firebase for testing)
try:
    import firebase_admin
    from firebase_admin import credentials, firestore
    FIREBASE_AVAILABLE = True
except ImportError:
    FIREBASE_AVAILABLE = False
    print("‚ö†Ô∏è Firebase not available - running in offline mode")

class ManualScraper:
    """Manual champion scraper for local execution"""

    def __init__(self):
        self.db = None
        self.scraper = LolalyticsBuildScraper()

        # Initialize Firebase if available and credentials exist
        if FIREBASE_AVAILABLE:
            self._init_firebase()

    def _init_firebase(self):
        """Initialize Firebase connection"""
        try:
            if not firebase_admin._apps:
                # Try environment variable first (GitHub Actions/serverless)
                cred_json = os.environ.get('FIREBASE_SERVICE_ACCOUNT_KEY')
                if cred_json:
                    cred = credentials.Certificate(json.loads(cred_json))
                else:
                    # Try local files
                    cred_paths = [
                        'firebase-key.json',
                        os.path.join(os.path.dirname(__file__), 'firebase-key.json'),
                        os.path.join(os.path.dirname(__file__), '..', 'firebase-key.json')
                    ]

                    cred = None
                    for path in cred_paths:
                        if os.path.exists(path):
                            cred = credentials.Certificate(path)
                            break

                    if not cred:
                        print("‚ö†Ô∏è No Firebase credentials found - running in offline mode")
                        return

                firebase_admin.initialize_app(cred)
                self.db = firestore.client()
                print("‚úÖ Firebase initialized")
        except Exception as e:
            print(f"‚ö†Ô∏è Firebase initialization failed: {e} - running in offline mode")

    def get_missing_champions(self):
        """Get champions that are not in the database"""
        if not self.db:
            print("‚ùå Cannot check missing champions without Firebase")
            return []

        try:
            # Get all champions from Riot API
            all_champions = get_champion_list()

            # Get champions already in database
            existing_champions = set()
            if self.db:
                champions_ref = self.db.collection('champions').document('all').collection('champions')
                existing_docs = champions_ref.stream()
                existing_champions = {doc.id for doc in existing_docs}

            # Find missing champions
            missing = [champ for champ in all_champions if champ.lower() not in existing_champions]

            print(f"Found {len(missing)} missing champions out of {len(all_champions)} total")
            return missing

        except Exception as e:
            print(f"‚ùå Error checking missing champions: {e}")
            return []

    def get_champions_missing_roles(self, role=None):
        """Get champions that exist but are missing from role containers"""
        if not self.db:
            print("‚ùå Cannot check missing roles without Firebase")
            return []

        try:
            roles_to_check = ['top', 'jungle', 'middle', 'bottom', 'support'] if role is None else [role]

            missing_by_role = {}

            for check_role in roles_to_check:
                # Get champions in this role container
                role_ref = self.db.collection('roles').document(check_role)
                role_doc = role_ref.get()

                role_champions = set()
                if role_doc.exists:
                    data = role_doc.to_dict()
                    role_champions = set(data.get('champions', []))

                # Get all champions that have this role in their data
                champions_ref = self.db.collection('champions').document('all').collection('champions')
                champions_with_role = []

                for doc in champions_ref.stream():
                    champ_data = doc.to_dict()
                    champ_roles = champ_data.get('roles', {})

                    # Check if champion has this role with meaningful presence
                    if check_role in champ_roles:
                        role_stats = champ_roles[check_role].get('stats', {})
                        pick_rate = role_stats.get('pick_rate', 0)
                        games = role_stats.get('games', 0)

                        # Include if they have any presence in this role
                        if pick_rate > 0 or games > 0:
                            champ_key = doc.id
                            if champ_key not in role_champions:
                                champions_with_role.append({
                                    'key': champ_key,
                                    'name': champ_data.get('name', champ_key),
                                    'pick_rate': pick_rate,
                                    'games': games
                                })

                if champions_with_role:
                    missing_by_role[check_role] = champions_with_role

            # Print summary
            total_missing = sum(len(champs) for champs in missing_by_role.values())
            if total_missing > 0:
                print(f"Found {total_missing} champions missing from role containers:")
                for role_name, champions in missing_by_role.items():
                    print(f"  {role_name}: {len(champions)} champions ({', '.join([c['name'] for c in champions[:3]])}...)")
            else:
                print("‚úÖ All champions are properly assigned to their role containers")

            return missing_by_role

        except Exception as e:
            print(f"‚ùå Error checking missing roles: {e}")
            import traceback
            traceback.print_exc()
            return {}

    def scrape_champion(self, champion_internal, target_patch=None, dry_run=False):
        """Scrape data for a single champion"""
        try:
            # Get champion display name
            champion_display = get_display_name(champion_internal)
            print(f"\n=== Processing {champion_internal} (display: {champion_display}) ===")

            if dry_run:
                print("üîç DRY RUN: Would scrape champion data")
                return True

            # Scrape League Wiki abilities data
            print("üìñ Scraping wiki abilities...")
            abilities_data = scrape_champion_abilities(champion_display)
            print(f"‚úÖ Found {len(abilities_data)} abilities")

            # Scrape Lolalytics build data
            patch_to_use = target_patch or get_current_patch()
            print(f"üìä Scraping lolalytics data (patch {patch_to_use})...")
            normalized_patch = normalize_patch_for_lolalytics(patch_to_use)
            build_data = self.scraper.scrape_champion_build(champion_display, patch=normalized_patch)

            # Get champion metadata
            champion_id = get_champion_id(champion_internal)
            champion_image_name = get_champion_image_name(champion_internal)

            # Combine the data
            combined_data = {
                'id': champion_id,
                'imageName': champion_image_name,
                'name': champion_display,
                'abilities': abilities_data,
                'lastUpdated': datetime.utcnow()
            }

            # Add lolalytics data if available
            if build_data:
                combined_data.update(build_data)
                print(f"‚úÖ Combined data: {len(build_data.get('roles', {}))} roles")
            else:
                print("‚ö†Ô∏è No build data available")

            # Store in Firebase if available
            if self.db:
                self._store_champion_data(champion_internal, combined_data)

            return True

        except Exception as e:
            print(f"‚ùå Error processing {champion_internal}: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _store_champion_data(self, champion_key, data):
        """Store champion data in Firebase"""
        try:
            doc_ref = self.db.collection('champions').document('all').collection('champions').document(champion_key.lower())
            doc_ref.set(data)
            print(f"üíæ Stored data for {champion_key}")
        except Exception as e:
            print(f"‚ùå Error storing data for {champion_key}: {e}")

    def update_role_containers(self):
        """Update role containers after manual scraping"""
        if not self.db:
            print("‚ö†Ô∏è Cannot update role containers without Firebase")
            return

        try:
            print("\nüîÑ Updating role containers...")

            # Import the update function from lambda_function
            from lambda_function import update_role_containers
            update_role_containers()

        except Exception as e:
            print(f"‚ùå Error updating role containers: {e}")

def main():
    parser = argparse.ArgumentParser(
        description="Manual Champion Scraper",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    parser.add_argument('champions', nargs='*', help='Champion names to scrape')
    parser.add_argument('--all', action='store_true', help='Scrape all champions')
    parser.add_argument('--missing', action='store_true', help='Scrape only missing champions')
    parser.add_argument('--missing-roles', nargs='?', const='all', help='Show champions missing from role containers (optionally specify role: top, jungle, middle, bottom, support)')
    parser.add_argument('--patch', help='Override patch version')
    parser.add_argument('--dry-run', action='store_true', help='Show what would be scraped without doing it')
    parser.add_argument('--update-roles', action='store_true', help='Update role containers after scraping')

    args = parser.parse_args()

    # Validate arguments
    if not args.champions and not args.all and not args.missing and not args.missing_roles:
        print("‚ùå Error: Must specify champions, --all, --missing, or --missing-roles")
        parser.print_help()
        sys.exit(1)

    # Initialize scraper (needed for all operations)
    scraper = ManualScraper()

    # Handle --missing-roles option (doesn't scrape, just reports)
    if args.missing_roles:
        if args.missing_roles == 'all':
            scraper.get_champions_missing_roles()
        else:
            # Validate role name
            valid_roles = ['top', 'jungle', 'middle', 'bottom', 'support']
            if args.missing_roles not in valid_roles:
                print(f"‚ùå Invalid role '{args.missing_roles}'. Valid roles: {', '.join(valid_roles)}")
                sys.exit(1)
            scraper.get_champions_missing_roles(args.missing_roles)
        return  # Exit after showing missing roles

    if args.all and args.missing:
        print("‚ùå Error: Cannot use --all and --missing together")
        sys.exit(1)

    if args.champions and (args.all or args.missing):
        print("‚ùå Error: Cannot specify champions with --all or --missing")
        sys.exit(1)

    # Determine which champions to scrape
    if args.all:
        champions_to_scrape = get_champion_list()
        print(f"üìã Will scrape all {len(champions_to_scrape)} champions")
    elif args.missing:
        champions_to_scrape = scraper.get_missing_champions()
        if not champions_to_scrape:
            print("‚ÑπÔ∏è No missing champions found")
            return
    else:
        # Use provided champion names - convert to internal keys
        champions_to_scrape = []
        all_champions = get_champion_list()

        for champ_input in args.champions:
            # Try to match input to champion
            matched = False
            for internal_key in all_champions:
                display_name = get_display_name(internal_key)
                if (champ_input.lower() == internal_key.lower() or
                    champ_input.lower() == display_name.lower() or
                    champ_input.lower() == encode_champion_name_for_lolalytics(display_name)):
                    champions_to_scrape.append(internal_key)
                    matched = True
                    break

            if not matched:
                print(f"‚ö†Ô∏è Champion '{champ_input}' not found, skipping")

        if not champions_to_scrape:
            print("‚ùå No valid champions specified")
            sys.exit(1)

    print(f"\nüöÄ Starting manual scrape of {len(champions_to_scrape)} champions...")

    if args.dry_run:
        print("üîç DRY RUN MODE - No actual scraping will be performed")

    # Scrape champions
    success_count = 0
    for i, champion in enumerate(champions_to_scrape):
        print(f"\n[{i+1}/{len(champions_to_scrape)}]")

        if scraper.scrape_champion(champion, args.patch, args.dry_run):
            success_count += 1

        # Rate limiting (skip for dry runs)
        if not args.dry_run and i < len(champions_to_scrape) - 1:
            wait_time = random.uniform(1, 3)
            print(f"‚è±Ô∏è Waiting {wait_time:.1f}s...")
            time.sleep(wait_time)

    print(f"\nüéâ Manual scraping completed! {success_count}/{len(champions_to_scrape)} champions processed successfully")

    # Update role containers if requested
    if args.update_roles and not args.dry_run:
        scraper.update_role_containers()

if __name__ == "__main__":
    main()
