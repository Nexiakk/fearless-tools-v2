import os
import json
import time
import requests
from .lolalytics_build_scraper import LolalyticsBuildScraper
from .wiki_scraper import scrape_champion_abilities
from datetime import datetime

# Firebase will be initialized only when needed
firebase_initialized = False
db = None

def init_firebase():
    """Initialize Firebase if not already done"""
    global firebase_initialized, db

    if firebase_initialized and db:
        return True

    try:
        import firebase_admin
        from firebase_admin import credentials, firestore

        if not firebase_admin._apps:
            # For GitHub Actions / serverless environments, credentials from environment variable
            cred_json = os.environ.get('FIREBASE_SERVICE_ACCOUNT_KEY')
            if cred_json:
                cred = credentials.Certificate(json.loads(cred_json))
            else:
                # Fallback for local testing - use firebase-key.json in current directory
                cred_path = 'firebase-key.json'
                if os.path.exists(cred_path):
                    cred = credentials.Certificate(cred_path)
                else:
                    # Try parent directory
                    cred_path = os.path.join(os.path.dirname(__file__), '..', '..', 'firebase-key.json')
                    if os.path.exists(cred_path):
                        cred = credentials.Certificate(cred_path)
                    else:
                        print("‚ùå No Firebase credentials found")
                        return False

            firebase_admin.initialize_app(cred)

        db = firestore.client()
        firebase_initialized = True
        return True

    except Exception as e:
        print(f"‚ùå Firebase initialization failed: {e}")
        return False

# Simple in-memory cache for Riot API data
riot_cache = {}

# Champion name mapping cache
champion_name_cache = {}
champion_data_cache = {}

def get_champion_data_mapping():
    """Get comprehensive champion data mapping from Riot API"""
    global champion_data_cache

    # Return cached mapping if available
    if champion_data_cache:
        return champion_data_cache

    try:
        # Get latest patch version
        current_patch = get_current_patch()

        # Fetch champion data from Riot API
        champions_url = f"https://ddragon.leagueoflegends.com/cdn/{current_patch}/data/en_US/champion.json"
        response = requests.get(champions_url, timeout=10)
        response.raise_for_status()
        champions_data = response.json()

        # Build comprehensive mapping: internal_key -> full data
        mapping = {}
        for key, champ_data in champions_data['data'].items():
            mapping[key] = {
                'id': int(champ_data['key']),  # Numeric champion ID
                'imageName': key,              # Internal string key for images
                'name': champ_data['name'],    # Display name
                'internalKey': key             # Internal key for reference
            }

        # Cache the mapping
        champion_data_cache.update(mapping)
        return mapping

    except Exception as e:
        print(f"Error fetching champion data mapping: {e}")
        return {}

def get_display_name(internal_key):
    """Convert Riot internal champion key to display name"""
    mapping = get_champion_data_mapping()
    return mapping.get(internal_key, {}).get('name', internal_key)

def get_champion_id(internal_key):
    """Get numeric champion ID from internal key"""
    mapping = get_champion_data_mapping()
    return mapping.get(internal_key, {}).get('id')

def get_champion_image_name(internal_key):
    """Get image name (internal key) for a champion"""
    mapping = get_champion_data_mapping()
    return mapping.get(internal_key, {}).get('imageName', internal_key)

def get_simplified_key(internal_key):
    """Generate simplified database key from internal key"""
    display_name = get_display_name(internal_key)
    return encode_champion_name_for_lolalytics(display_name)

def encode_champion_name_for_wiki(display_name):
    """Encode champion name for League Wiki URL format"""
    # First replace spaces with underscores
    encoded = display_name.replace(' ', '_')

    # Then URL encode any remaining special characters (like apostrophes)
    # Use quote() but allow underscores to remain
    from urllib.parse import quote
    encoded = quote(encoded, safe='_')

    return encoded

def encode_champion_name_for_lolalytics(display_name):
    """Encode champion name for Lolalytics URL format (simplified, no special chars/spaces)"""
    import re

    # Convert to lowercase
    encoded = display_name.lower()

    # Remove apostrophes, quotes, and other special characters
    encoded = re.sub(r"['\"]", '', encoded)

    # Replace spaces with nothing (remove spaces)
    encoded = encoded.replace(' ', '')

    # Handle roman numerals (IV -> iv, etc.)
    # But keep them as is since they're already lowercase

    return encoded

def get_champion_list():
    """Get list of all champions from Riot Data Dragon API"""
    try:
        # Get latest version
        current_patch = get_current_patch()

        # Get champion data
        champions_url = f"https://ddragon.leagueoflegends.com/cdn/{current_patch}/data/en_US/champion.json"
        champions_response = requests.get(champions_url, timeout=10)
        champions_response.raise_for_status()
        champions_data = champions_response.json()

        # Return sorted list of champion names
        champion_names = list(champions_data['data'].keys())
        champion_names.sort()
        return champion_names

    except Exception as e:
        print(f"Error fetching champion list from Riot API: {e}")
        # Fallback to a smaller static list for testing
        return [
            'Aatrox', 'Ahri', 'Akali', 'Ashe', 'Jinx', 'Lux', 'Miss Fortune', 'Vayne', 'Yuumi',
            'Yasuo', 'Zed', 'Kaisa', 'Caitlyn', 'Ezreal', 'Varus'
        ]

def normalize_patch_for_lolalytics(patch_version):
    """Convert Riot API patch format (x.y.z) to Lolalytics format (x.y)"""
    # Split by dots and take only first two components
    parts = patch_version.split('.')
    if len(parts) >= 2:
        return f"{parts[0]}.{parts[1]}"
    return patch_version

def check_patch_viability(patch_version):
    """
    Check if current patch has sufficient sample size by scraping Lolalytics tierlist.
    Returns tuple: (use_current_patch, target_patch, metrics)
    """
    try:
        print(f"üîç Checking patch {patch_version} viability...")

        # Convert patch format for Lolalytics (remove .1 suffix)
        lolalytics_patch = normalize_patch_for_lolalytics(patch_version)

        # Scrape Lolalytics tierlist for this patch
        url = f"https://lolalytics.com/pl/lol/tierlist/?patch={lolalytics_patch}&tier=diamond_plus"
        response = requests.get(url, timeout=15)
        response.raise_for_status()

        # Parse HTML to extract champion data
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')

        champions_data = []

        # Find champion rows in the tierlist table
        champion_rows = soup.find_all('tr', class_='ChampionRow')

        for row in champion_rows:
            try:
                # Extract champion name
                name_elem = row.find('a', class_='ChampionName')
                if not name_elem:
                    continue
                champion_name = name_elem.get_text().strip()

                # Extract games count (usually in a span with specific class)
                games_elem = row.find('span', string=lambda text: text and 'games' in text.lower())
                if games_elem:
                    games_text = games_elem.get_text().strip()
                    # Parse number (e.g., "16,947 games" -> 16947)
                    games_count = int(games_text.replace(',', '').replace(' games', ''))
                else:
                    # Try alternative selectors for games count
                    stats_div = row.find('div', class_='ChampionStats')
                    if stats_div:
                        games_text = stats_div.get_text()
                        # Look for number pattern
                        import re
                        match = re.search(r'(\d{1,3}(?:,\d{3})*)', games_text)
                        if match:
                            games_count = int(match.group(1).replace(',', ''))
                        else:
                            continue
                    else:
                        continue

                champions_data.append({
                    'name': champion_name,
                    'games': games_count
                })

            except Exception as e:
                print(f"‚ö†Ô∏è Error parsing champion row: {e}")
                continue

        if not champions_data:
            print("‚ö†Ô∏è No champion data found in tierlist")
            return False, get_previous_patch(patch_version), {}

        # Sort by games descending and calculate metrics
        champions_data.sort(key=lambda x: x['games'], reverse=True)

        # Calculate top 10 total games
        top_10_total = sum(champ['games'] for champ in champions_data[:10])

        # Calculate average games per champion
        avg_games = sum(champ['games'] for champ in champions_data) / len(champions_data)

        metrics = {
            'champion_count': len(champions_data),
            'top_10_total_games': top_10_total,
            'avg_games_per_champion': avg_games,
            'top_10_champions': champions_data[:10]
        }

        # Check thresholds
        top_10_threshold = 200000  # 200k
        avg_threshold = 5000       # 5k

        viable = (top_10_total >= top_10_threshold and avg_games >= avg_threshold)

        target_patch = patch_version if viable else get_previous_patch(patch_version)

        print("üìä Patch viability metrics:")
        print(f"  - Champions analyzed: {len(champions_data)}")
        print(f"  - Top 10 total games: {top_10_total:,} (threshold: {top_10_threshold:,})")
        print(f"  - Average games/champion: {avg_games:.0f} (threshold: {avg_threshold})")
        print(f"  - Result: {'‚úÖ VIABLE' if viable else '‚ùå FALLBACK'}")

        if not viable:
            print(f"  - Fallback to patch: {target_patch}")

        return viable, target_patch, metrics

    except Exception as e:
        print(f"‚ùå Error checking patch viability: {e}")
        import traceback
        traceback.print_exc()
        # On error, fall back to previous patch
        return False, get_previous_patch(patch_version), {}

def scrape_and_store_data():
    """Main function to scrape data and store in Firebase"""
    print("Starting data scraping...")

    # Get current patch and check viability
    current_patch = get_current_patch()
    print(f"Current patch: {current_patch}")

    # Check if current patch has sufficient sample size
    use_current, target_patch, viability_metrics = check_patch_viability(current_patch)

    if use_current:
        print(f"‚úÖ Using current patch {current_patch} for scraping")
    else:
        print(f"‚ö†Ô∏è Current patch {current_patch} has insufficient data")
        print(f"üîÑ Falling back to patch {target_patch}")

    champions = get_champion_list()

    # Process each champion
    for champion_internal in champions:
        try:
            # Get all champion data
            champion_id = get_champion_id(champion_internal)
            champion_display = get_display_name(champion_internal)
            champion_image_name = get_champion_image_name(champion_internal)
            simplified_key = get_simplified_key(champion_internal)

            print(f"\n=== Processing {champion_internal} (display: {champion_display}, key: {simplified_key}) ===")

            # Scrape League Wiki abilities data
            print(f"Scraping wiki abilities for {champion_display}...")
            abilities_data = scrape_champion_abilities(champion_display)
            print(f"Found {len(abilities_data)} abilities")

            # Scrape Lolalytics build data with target patch
            print(f"Scraping lolalytics data for {champion_display} (patch {target_patch})...")
            lolalytics_scraper = LolalyticsBuildScraper()
            normalized_patch = normalize_patch_for_lolalytics(target_patch)
            build_data = lolalytics_scraper.scrape_champion_build(champion_display, patch=normalized_patch)

            # Combine the data with new structure
            combined_data = {
                'id': champion_id,              # Numeric champion ID (136 for Aurelion Sol)
                'imageName': champion_image_name, # Internal key for images (AurelionSol)
                'name': champion_display,       # Display name (Aurelion Sol)
                'abilities': abilities_data,
                'lastUpdated': datetime.utcnow()
            }

            # Add lolalytics data if available (flattened structure, remove tier field)
            if build_data:
                # Copy build data but exclude the 'tier' field since it's always diamond_plus
                for key, value in build_data.items():
                    if key != 'tier':  # Skip the tier field
                        combined_data[key] = value
                print(f"Combined data: {len(build_data.get('roles', {}))} roles")
            else:
                print("No build data available")

            # Store combined data using internal key (remove /data subdocument)
            store_combined_champion_data(champion_internal, combined_data)
            print(f"‚úÖ Successfully stored data for {champion_display} (key: {champion_internal})")

        except Exception as e:
            print(f"‚ùå Error processing {champion_internal}: {e}")
            import traceback
            traceback.print_exc()

    # Update role containers for optimized queries
    print("\nüîÑ Updating role containers...")
    update_role_containers()

    # Clean up old patch data to save space
    print("\nüßπ Cleaning up old patch data...")
    cleanup_old_patch_data()

    print("\nüéâ Data scraping, optimization, and cleanup completed!")

class SmartUpdateEngine:
    """Intelligent update system for champion data"""

    def __init__(self):
        self.tier_thresholds = {
            'S': {'min_percent': 0.4, 'min_absolute': 15000},  # 40% of old or 15K
            'A': {'min_percent': 0.5, 'min_absolute': 10000},  # 50% of old or 10K
            'B': {'min_percent': 0.6, 'min_absolute': 7500},   # 60% of old or 7.5K
            'C': {'min_percent': 0.7, 'min_absolute': 5000},   # 70% of old or 5K
        }

    def calculate_champion_tier(self, total_games):
        """Categorize champion by historical play rate"""
        if total_games >= 150000:
            return 'S'
        elif total_games >= 75000:
            return 'A'
        elif total_games >= 25000:
            return 'B'
        else:
            return 'C'

    def calculate_adaptive_threshold(self, old_total_games):
        """Calculate switching threshold based on champion's tier"""
        tier = self.calculate_champion_tier(old_total_games)
        config = self.tier_thresholds[tier]

        return max(
            old_total_games * config['min_percent'],
            config['min_absolute']
        )

    def should_update_champion(self, current_data, new_data):
        """Simplified: Patch viability already checked globally"""

        # Always update on patch changes (patch already validated globally as ready)
        if new_data.get('patch') != current_data.get('patch'):
            return {
                'update': True,
                'abilities': True,
                'lolalytics': True,
                'reason': f"Patch changed to {new_data['patch']} (validated globally)"
            }

        # Same patch: Only update if abilities changed
        else:
            abilities_changed = self._abilities_changed(
                current_data.get('abilities', []),
                new_data.get('abilities', [])
            )
            return {
                'update': abilities_changed,  # Only if abilities changed
                'abilities': abilities_changed,
                'lolalytics': True,  # Always update (growing sample)
                'reason': f"Same patch: abilities={abilities_changed}, lolalytics=True"
            }

    def _calculate_total_games(self, data):
        """Sum games across all roles"""
        return sum(
            role_data.get('stats', {}).get('games', 0)
            for role_data in data.get('roles', {}).values()
        )

    def _abilities_changed(self, old_abilities, new_abilities):
        """Check if abilities actually changed"""
        if len(old_abilities) != len(new_abilities):
            return True

        # Compare each ability
        for old, new in zip(old_abilities, new_abilities):
            if (old.get('name') != new.get('name') or
                old.get('cooldown') != new.get('cooldown') or
                old.get('type') != new.get('type')):
                return True

        return False

    def get_viable_roles(self, scraped_data, historical_roles=None):
        """Get all roles that should be stored - simplified since we only scrape ‚â•9% roles"""
        viable_roles = set()

        # All scraped roles are already ‚â•9% pickrate, so just return them
        viable_roles.update(scraped_data.get('roles', {}).keys())

        # Add historically viable roles (if any exist from before the optimization)
        if historical_roles:
            viable_roles.update(historical_roles)

        return list(viable_roles)

def test_data_integration():
    """Test the data integration with smart updates"""
    print("üß™ Testing data integration with smart updates...")

    champion = "Aatrox"
    update_engine = SmartUpdateEngine()

    try:
        print(f"\n=== Testing {champion} Integration ===")

        # Scrape League Wiki abilities data
        print("Scraping wiki abilities...")
        abilities_data = scrape_champion_abilities(champion)
        print(f"‚úÖ Found {len(abilities_data)} abilities")

        # Scrape Lolalytics build data
        print("Scraping lolalytics data...")
        lolalytics_scraper = LolalyticsBuildScraper()
        build_data = lolalytics_scraper.scrape_champion_build(champion.lower())

        # Simulate current data (what would be in database)
        current_data = {
            'name': champion,
            'abilities': abilities_data,  # Same abilities for testing
            'patch': '15.23',  # Simulate old patch
            'tier': 'diamond_plus',
            'roles': {
                'top': {'stats': {'games': 95000, 'pick_rate': 45.0}},
                'jungle': {'stats': {'games': 75000, 'pick_rate': 38.0}}
            }
        }

        # Decide what to update
        update_decision = update_engine.should_update_champion(current_data, build_data)

        print(f"\nüìä Update Decision: {update_decision}")

        # Apply selective updates
        final_data = current_data.copy()  # Start with current

        if update_decision['abilities']:
            final_data['abilities'] = abilities_data

        if update_decision['lolalytics']:
            # Update with new build data
            final_data.update(build_data)
            # Filter to viable roles only
            viable_roles = update_engine.get_viable_roles(build_data)
            final_data['roles'] = {
                role: final_data['roles'][role]
                for role in viable_roles
                if role in final_data['roles']
            }

        # Show results
        print("\nüìà Final Data Summary:")
        print(f"  - Abilities: {len(final_data.get('abilities', []))} abilities")
        print(f"  - Patch: {final_data.get('patch')} (was {current_data.get('patch')})")
        print(f"  - Roles: {list(final_data.get('roles', {}).keys())}")
        print(f"  - Total Games: {update_engine._calculate_total_games(final_data)}")

        champion_tier = update_engine.calculate_champion_tier(
            update_engine._calculate_total_games(current_data)
        )
        threshold = update_engine.calculate_adaptive_threshold(
            update_engine._calculate_total_games(current_data)
        )
        print(f"  - Champion Tier: {champion_tier} (threshold: {threshold})")

        print("\n‚úÖ Smart integration test successful!")
        return final_data, update_decision

    except Exception as e:
        print(f"‚ùå Integration test failed: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def store_combined_champion_data(champion_key: str, data: dict):
    """Store combined champion data and update role containers incrementally"""
    if not init_firebase():
        raise Exception("Firebase not available")

    # Store champion data
    doc_ref = db.collection('champions').document(f'all/{champion_key}')
    doc_ref.set(data)

    # Update role containers for this champion
    update_role_containers_for_champion(champion_key, data)

def update_role_containers_for_champion(champion_key: str, champion_data: dict):
    """Update role containers for a single champion"""
    roles = champion_data.get('roles', {})

    for role_name, role_stats in roles.items():
        if role_name and role_stats.get('stats', {}).get('games', 0) > 0:
            # Normalize role name for storage
            normalized_role = normalize_role_name(role_name)
            role_ref = get_role_container_ref(normalized_role)

            # Update role container atomically
            update_role_container_incremental(role_ref, champion_key, role_stats)

def update_role_container_incremental(role_ref, champion_key: str, role_stats: dict):
    """Atomically update a single role container"""
    @firestore.transactional
    def update_in_transaction(transaction):
        role_doc = role_ref.get(transaction=transaction)

        if role_doc.exists:
            current_data = role_doc.to_dict()
            champions_list = current_data.get('champions', [])
        else:
            champions_list = []

        # Add champion if not already present
        if champion_key not in champions_list:
            champions_list.append(champion_key)

        # Update role document
        transaction.set(role_ref, {
            'champions': champions_list,
            'count': len(champions_list),
            'lastUpdated': datetime.utcnow()
        }, merge=True)

    update_in_transaction(db)

def get_role_container_ref(role_name: str):
    """Get reference to role container document at champions/roles/{roleName}"""
    return db.collection('champions').document(f'roles/{role_name}')

def normalize_role_name(role_name: str) -> str:
    """Normalize role names for storage"""
    mapping = {
        'top': 'top',
        'jungle': 'jungle',
        'mid': 'middle',
        'bot': 'bottom',
        'adc': 'bottom',
        'support': 'support'
    }
    return mapping.get(role_name.lower(), role_name.lower())

def update_role_containers():
    """Create optimized role container indexes for Firebase free tier"""
    if not init_firebase():
        print("Firebase not available, skipping role container update")
        return

    print("Updating role containers for optimized Firebase queries...")

    try:
        # Get all champions
        champions_ref = db.collection('champions')
        champions = champions_ref.stream()

        role_champions = {
            'top': [],
            'jungle': [],
            'middle': [],
            'bottom': [],
            'support': []
        }

        for champ_doc in champions:
            champ_data = champ_doc.to_dict()

            # Check which roles this champion plays
            roles = champ_data.get('roles', {})
            for role in roles:
                if role in role_champions:
                    role_champions[role].append({
                        'id': champ_doc.id,
                        'name': champ_data.get('name', ''),
                        'pickRate': roles[role].get('stats', {}).get('pick_rate', 0)
                    })

        # Sort champions by pick rate (highest first) and limit if needed
        for role in role_champions:
            role_champions[role].sort(key=lambda x: x['pickRate'], reverse=True)
            # Keep all champions for now - Firebase can handle it
            # Could limit to top N if storage becomes an issue

        # Store role containers
        roles_ref = db.collection('roles')
        current_patch = None

        for role, champions_list in role_champions.items():
            # Extract just champion IDs for lightweight queries
            champion_ids = [champ['id'] for champ in champions_list]

            # Get current patch from first champion if available
            if not current_patch and champions_list:
                first_champ_data = db.collection('champions').document(f'all/{champions_list[0]["id"]}').get()
                if first_champ_data.exists:
                    current_patch = first_champ_data.to_dict().get('patch')

            role_doc = {
                'champions': champion_ids,  # Lightweight: just IDs
                'count': len(champion_ids),
                'lastUpdated': datetime.utcnow()
            }

            if current_patch:
                role_doc['patch'] = current_patch

            roles_ref.document(role).set(role_doc)
            print(f"‚úÖ Updated {role}: {len(champion_ids)} champions")

        print(f"üéâ Role containers updated successfully!")

    except Exception as e:
        print(f"‚ùå Error updating role containers: {e}")
        import traceback
        traceback.print_exc()

def get_riot_versions():
    """Fetch and cache Riot API versions for automatic patch detection"""
    cache_key = "riot_versions"
    cached = riot_cache.get(cache_key)

    if cached and time.time() - cached['timestamp'] < 3600:  # 1 hour cache
        return cached['versions']

    try:
        print("Fetching Riot API versions...")
        response = requests.get("https://ddragon.leagueoflegends.com/api/versions.json", timeout=10)
        response.raise_for_status()
        versions = response.json()

        # Cache the result
        riot_cache[cache_key] = {
            'versions': versions,
            'timestamp': time.time()
        }

        print(f"‚úÖ Fetched {len(versions)} patch versions from Riot API")
        return versions

    except Exception as e:
        print(f"‚ùå Failed to fetch Riot versions: {e}")
        # Return cached version if available, otherwise empty list
        return cached['versions'] if cached else []

def get_previous_patch(current_patch):
    """Automatically find the previous patch using Riot API data"""
    versions = get_riot_versions()

    try:
        current_index = versions.index(current_patch)
        if current_index + 1 < len(versions):
            previous_patch = versions[current_index + 1]  # Next item is previous (list is newest first)
            print(f"‚úÖ Detected previous patch: {current_patch} ‚Üí {previous_patch}")
            return previous_patch
        else:
            print(f"‚ö†Ô∏è No previous patch found for {current_patch}")
    except ValueError:
        print(f"‚ö†Ô∏è Current patch {current_patch} not found in Riot versions")

    return None

def get_champion_fallback_data(champion_id, current_patch):
    """Get fallback data from automatically detected previous patch"""
    if not init_firebase():
        return None

    previous_patch = get_previous_patch(current_patch)
    if not previous_patch:
        return None

    try:
        # Look for archived data from previous patch
        fallback_doc = db.collection('champions') \
                        .document(f'all/{champion_id}') \
                        .collection('patch_history') \
                        .document(previous_patch) \
                        .get()

        if fallback_doc.exists:
            data = fallback_doc.to_dict()
            data['_fallback'] = True
            data['_fallback_patch'] = previous_patch
            data['_current_patch'] = current_patch
            print(f"‚úÖ Found fallback data for {champion_id} from patch {previous_patch}")
            return data
        else:
            print(f"‚ö†Ô∏è No archived data found for {champion_id} in patch {previous_patch}")

    except Exception as e:
        print(f"‚ùå Error retrieving fallback data: {e}")

    return None

def cleanup_old_patch_data():
    """Clean up old patch data, keeping only current + 1 previous patch"""
    if not init_firebase():
        print("Firebase not available, skipping cleanup")
        return

    print("üßπ Cleaning up old patch data...")

    try:
        # Get current patch from a sample champion
        current_patch = None
        champions_ref = db.collection('champions')
        sample_champions = champions_ref.limit(1).get()

        for doc in sample_champions:
            current_patch = doc.to_dict().get('patch')
            break

        if not current_patch:
            print("‚ö†Ô∏è Could not determine current patch, skipping cleanup")
            return

        print(f"Current active patch: {current_patch}")

        # Collect all patch history data
        old_patches = []
        champions_stream = champions_ref.stream()

        for champ_doc in champions_stream:
            patch_history_ref = champ_doc.collection('patch_history')
            patches = patch_history_ref.list_documents()

            for patch_doc in patches:
                patch_version = patch_doc.id
                if patch_version != current_patch:
                    old_patches.append((champ_doc.id, patch_version, patch_doc))

        # Group by patch version
        patches_by_version = {}
        for champ_id, patch_version, doc_ref in old_patches:
            if patch_version not in patches_by_version:
                patches_by_version[patch_version] = []
            patches_by_version[patch_version].append(doc_ref)

        # Sort patches by version (newest first)
        sorted_patches = sorted(patches_by_version.keys(), reverse=True)

        # Keep only the most recent previous patch
        if len(sorted_patches) > 1:
            patches_to_delete = sorted_patches[1:]  # Everything except the newest

            total_deleted = 0
            for old_patch in patches_to_delete:
                print(f"üóëÔ∏è Deleting old patch data: {old_patch}")
                for doc_ref in patches_by_version[old_patch]:
                    doc_ref.delete()
                    total_deleted += 1

            print(f"‚úÖ Cleanup complete: deleted {total_deleted} old patch documents")
        else:
            print("‚ÑπÔ∏è No old patch data to clean up")

    except Exception as e:
        print(f"‚ùå Error during cleanup: {e}")
        import traceback
        traceback.print_exc()

def get_current_patch():
    """Get the current League of Legends patch version from Riot API"""
    try:
        versions_url = "https://ddragon.leagueoflegends.com/api/versions.json"
        versions_response = requests.get(versions_url, timeout=10)
        versions_response.raise_for_status()
        versions = versions_response.json()
        return versions[0]  # Latest version is first in the list
    except Exception as e:
        print(f"Error fetching current patch from Riot API: {e}")
        return "15.24"  # Fallback to known recent patch

def get_champion_list():
    """Get list of all champions from Riot Data Dragon API"""
    try:
        # Get latest version
        current_patch = get_current_patch()

        # Get champion data
        champions_url = f"https://ddragon.leagueoflegends.com/cdn/{current_patch}/data/en_US/champion.json"
        champions_response = requests.get(champions_url, timeout=10)
        champions_response.raise_for_status()
        champions_data = champions_response.json()

        # Return sorted list of champion names
        champion_names = list(champions_data['data'].keys())
        champion_names.sort()
        return champion_names

    except Exception as e:
        print(f"Error fetching champion list from Riot API: {e}")
        # Fallback to a smaller static list for testing
        return [
            'Aatrox', 'Ahri', 'Akali', 'Ashe', 'Jinx', 'Lux', 'Miss Fortune', 'Vayne', 'Yuumi',
            'Yasuo', 'Zed', 'Kaisa', 'Caitlyn', 'Ezreal', 'Varus'
        ]

def test_name_mapping():
    """Test the champion name mapping and encoding functions"""
    print("üß™ Testing champion name mapping and encoding...")

    test_cases = [
        ('MonkeyKing', 'Wukong'),
        ('KSante', "K'Sante"),
        ('JarvanIV', 'Jarvan IV'),
        ('Aatrox', 'Aatrox'),
        ('MissFortune', 'Miss Fortune')
    ]

    print("\nüìã Testing name mapping:")
    for internal, expected_display in test_cases:
        actual_display = get_display_name(internal)
        status = "‚úÖ" if actual_display == expected_display else "‚ùå"
        print(f"  {status} {internal} ‚Üí {actual_display} (expected: {expected_display})")

    print("\nüîó Testing URL encoding:")

    encoding_tests = [
        ("K'Sante", "K%27Sante", "ksante"),
        ("Jarvan IV", "Jarvan_IV", "jarvaniv"),
        ("Wukong", "Wukong", "wukong"),
        ("Miss Fortune", "Miss_Fortune", "missfortune")
    ]

    for display_name, expected_wiki, expected_lolalytics in encoding_tests:
        actual_wiki = encode_champion_name_for_wiki(display_name)
        actual_lolalytics = encode_champion_name_for_lolalytics(display_name)

        wiki_status = "‚úÖ" if actual_wiki == expected_wiki else "‚ùå"
        lolalytics_status = "‚úÖ" if actual_lolalytics == expected_lolalytics else "‚ùå"

        print(f"  {display_name}:")
        print(f"    Wiki: {wiki_status} {actual_wiki} (expected: {expected_wiki})")
        print(f"    Lolalytics: {lolalytics_status} {actual_lolalytics} (expected: {expected_lolalytics})")

    print("\n‚úÖ Name mapping tests completed!")

if __name__ == "__main__":
    # Test the name mapping first
    test_name_mapping()

    # Test the integration
    test_result = test_data_integration()

    # Uncomment to run full scraping with Firebase
    # scrape_and_store_data()
